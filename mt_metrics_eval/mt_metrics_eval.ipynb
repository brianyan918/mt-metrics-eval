{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGuFoP_Gq9X9"
      },
      "source": [
        "This is a demo colab for MTME. It assumes you have mt_metrics_eval installed on your runtime, and have downloaded the data onto that machine. Run the cells below in order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH8o8UKmUhQ8"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "Cr0TM9EY7wOH"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "\n",
        "from mt_metrics_eval import meta_info\n",
        "from mt_metrics_eval import data\n",
        "from mt_metrics_eval import stats\n",
        "from mt_metrics_eval import tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "GznnWylA8gwJ"
      },
      "outputs": [],
      "source": [
        "# @title Print all available evalsets\n",
        "\n",
        "for testset in meta_info.DATA:\n",
        "  print(f'{testset}:', ' '.join(lp for lp in meta_info.DATA[testset]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfiSe4Yt8sz3"
      },
      "outputs": [],
      "source": [
        "# @title Load data for WMT21 language pairs scored with MQM\n",
        "\n",
        "all_evs = {}  # name/lp -\u003e evs\n",
        "for testset in meta_info.DATA:\n",
        "  if not testset.startswith('wmt21'): continue\n",
        "  for lp in meta_info.DATA[testset]:\n",
        "    if 'mqm' in meta_info.DATA[testset][lp].std_gold.values():\n",
        "      all_evs[f'{testset}/{lp}'] = data.EvalSet(testset, lp, True)\n",
        "\n",
        "print('\\n'.join(all_evs.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "jAuNRBA79Yai"
      },
      "outputs": [],
      "source": [
        "# @title Print summaries for all loaded evalsets\n",
        "\n",
        "print(f'{\"name\":\u003c20}  segs sys metrics gold  refs std')\n",
        "for name, evs in all_evs.items():\n",
        "  nsegs = len(evs.src)\n",
        "  nsys = len(evs.sys_names)\n",
        "  nmetrics = len(evs.metric_basenames)\n",
        "  gold = evs.StdHumanScoreName('sys')\n",
        "  nrefs = len(evs.ref_names)\n",
        "  std_ref = evs.std_ref\n",
        "\n",
        "  print(f'{name:\u003c20} {nsegs:5d} {nsys:3d} {nmetrics:7d} '\n",
        "        f'{gold:5} {nrefs:4d} {std_ref}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW9Mda-jUpqh"
      },
      "source": [
        "# Comparing metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSwjndy3mQeo"
      },
      "outputs": [],
      "source": [
        "# @title Set up for comparing metrics\n",
        "\n",
        "# There are many different ways to evaluate the performance of MT metrics. The\n",
        "# most obvious question is what correlation statistic we should use to capture\n",
        "# the similarity between a vector of metric scores and a vector of gold scores\n",
        "# (human ratings). A less obvious question is where those vectors come from.\n",
        "# We'll defer the choice of correlation statistic to later cells, and begin\n",
        "# by setting some parameters that precisely define the vectors we're interested\n",
        "# in comparing.\n",
        "\n",
        "# Use all evalsets that we've loaded.\n",
        "evs_list = all_evs.values()\n",
        "\n",
        "# Choose the version of each metric that uses the standard reference for each\n",
        "# evalset.\n",
        "main_refs = [{evs.std_ref} for evs in evs_list]\n",
        "\n",
        "# Some alternative references are known to be close to the standard reference.\n",
        "# Don't include these among systems to be scored if we are including 'human'\n",
        "# systems. The only currently known instance is refB in wmt21.news/en-de,\n",
        "# which is similar to the standard refC.\n",
        "close_refs = [{'refB'} if k == 'wmt21.news/en-de' else set() for k in all_evs]\n",
        "\n",
        "# Include 'human' systems (ie, reference translations) among systems to be\n",
        "# scored. This can make the task more challenging, since some metrics are\n",
        "# biased against less literal references.\n",
        "include_human = True\n",
        "\n",
        "# Don't include systems considered to be outliers. These are systems that are\n",
        "# much better or worse than all other systems, so they are easy for all metrics\n",
        "# to rank correctly).\n",
        "include_outliers = False\n",
        "\n",
        "# Use MQM ratings as gold scores rather than the scores provided by the main\n",
        "# WMT task. Metrics tasks have used MQM for main results since 2021.\n",
        "gold_name = 'mqm'\n",
        "\n",
        "# Only compare metrics that have been designated as primary submissions. This\n",
        "# removes metric variants that are similar to each other, and reduces the size\n",
        "# of the comparison matrix.\n",
        "primary_metrics = True\n",
        "\n",
        "# Don't limit the results to a particular domain. In WMT21, domains are treated\n",
        "# as separate test-sets, so this is a no-op (WMT22 is a different story).\n",
        "domain = None\n",
        "\n",
        "# Set the number of resampling runs for determining whether one metric is better\n",
        "# than another according to the permutation test. We'll use 5 to make the demo\n",
        "# finish quickly, but at least 1000 is required for stable results.\n",
        "k = 5\n",
        "\n",
        "# Set the size of blocks for 'early stopping' checks during resampling. If\n",
        "# you're using k = 1000, this can speed up the computation, usually with\n",
        "# only minimal changes to the results.\n",
        "psd = stats.PermutationSigDiffParams(block_size = 100)\n",
        "\n",
        "# Set the p-value for deciding wheter metrics are considered to be significantly\n",
        "# different. Lower values make the test more stringent.\n",
        "pval = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffPW_P5yxMbu"
      },
      "outputs": [],
      "source": [
        "# @title Evaluate metrics using global accuracy\n",
        "\n",
        "# Global accuracy, introduced by Kocmi et al (https://arxiv.org/abs/2107.10821)\n",
        "# is a robust way to evaluate the performance of a metric across many different\n",
        "# settings. The idea is to count the number of pairwise system rankings where\n",
        "# the metric agrees with the gold ranking, and micro average this across all\n",
        "# settings.\n",
        "\n",
        "# The output shows the rank of each metric's significance cluster, followed\n",
        "# by its accuracy, and whether it is statistically tied with (=) or better than\n",
        "# (\u003e) each lower-ranking metric.\n",
        "\n",
        "\n",
        "ranks, matrix = data.CompareMetricsWithGlobalAccuracy(\n",
        "    evs_list, main_refs, close_refs, include_human, include_outliers,\n",
        "    gold_name, primary_metrics, domain, k, psd, pval)\n",
        "\n",
        "data.PrintMetricComparison(ranks, matrix, pval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCXij-xmO_E9"
      },
      "outputs": [],
      "source": [
        "# @title Evaluate metrics using system-level Pearson correlation\n",
        "\n",
        "# Pearson correlation measures the degree of linear correspondence between\n",
        "# metric and gold scores. Computing a single correlation across different\n",
        "# evalsets isn't a great idea, so the interface forces you to choose a single\n",
        "# set. We'll pick 'wmt21.news/en-de'. The part of the computation that extracts\n",
        "# relevant score vectors is factored into a separate step to allow you to\n",
        "# compute other correlations with these vectors.\n",
        "\n",
        "# Notice that the ranking is quite different from the accuracy ranking, partly\n",
        "# because we're using only a subset of the data, and partly because Pearson and\n",
        "# accuracy measure different things. The ranking also includes two metrics that\n",
        "# were automatically filtered out of the accuracy ranking because they weren't\n",
        "# available for all evalsets.\n",
        "\n",
        "evs = all_evs['wmt21.news/en-de']\n",
        "corrs = data.GetCorrelations(\n",
        "    evs, 'sys', {evs.std_ref}, {'refB'}, include_human, include_outliers,\n",
        "    gold_name, primary_metrics, domain)\n",
        "ranks, matrix = data.CompareMetrics(\n",
        "    corrs, scipy.stats.pearsonr, 'none', k, psd, pval)\n",
        "\n",
        "data.PrintMetricComparison(ranks, matrix, pval, evs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ur6XTs9hlmG"
      },
      "outputs": [],
      "source": [
        "# @title Evaluate metrics using segment-level Kendall correlation\n",
        "\n",
        "# Kendall correlation is similar to pairwise accuracy, except that it is\n",
        "# normalized differently. The function calls are identical to the previous one,\n",
        "# except that we set the 'level' parameter to 'seg', and specify Kendall rather\n",
        "# than Pearson. The value of the 'average_by' parameter also matters here, as it\n",
        "# specifies how system x segment score matrices get converted into vectors for\n",
        "# comparison. We will use 'none', which just flattens the matrices.\n",
        "\n",
        "# The resulting ranking is similar to the ranking from accuracy. One noticeable\n",
        "# difference is that the significance clusters are smaller because they are\n",
        "# based on more data (much larger vectors). Notice that BLEU is absent because\n",
        "# it isn't available at the segment level.\n",
        "\n",
        "evs = all_evs['wmt21.news/en-de']\n",
        "corrs = data.GetCorrelations(\n",
        "    evs, 'seg', {evs.std_ref}, {'refB'}, include_human, include_outliers,\n",
        "    gold_name, primary_metrics, domain)\n",
        "ranks, matrix = data.CompareMetrics(\n",
        "    corrs, scipy.stats.kendalltau, 'none', k, psd, pval)\n",
        "\n",
        "data.PrintMetricComparison(ranks, matrix, pval, evs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1P8jUG0kUEP"
      },
      "outputs": [],
      "source": [
        "# @title Evaluate metrics using seg-level accuracy with optimized tie threshold.\n",
        "\n",
        "# This is an implementation of the acc*_eq pairwise ranking accuracy proposed in\n",
        "# https://arxiv.org/abs/2305.14324. This is similar to global accuracy, but it\n",
        "# additionally gives metrics credit for predicting ties in gold scores, which\n",
        "# arise frequently in MQM segment-level data. To avoid bias due to differences\n",
        "# in scoring precision for different metrics, an optimal threshold for assigning\n",
        "# ties is automatically computed for each metric and test set.\n",
        "\n",
        "# For demo purposes we disable significance testing by setting k to 0.\n",
        "# (Significance testing works but is currently very slow.) Note that the\n",
        "# optimization procedure uses sampling, so results can change across different\n",
        "# runs.\n",
        "\n",
        "evs = all_evs['wmt21.news/en-de']\n",
        "corrs = data.GetCorrelations(\n",
        "    evs, 'seg', {evs.std_ref}, {'refB'}, include_human, include_outliers,\n",
        "    gold_name, primary_metrics, domain)\n",
        "ranks, matrix = data.CompareMetrics(\n",
        "    corrs, stats.KendallWithTiesOpt, 'item', 0, psd, pval, variant='acc23',\n",
        "    sample_rate=0.1)\n",
        "\n",
        "data.PrintMetricComparison(ranks, matrix, pval, evs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z304g56JBgq0"
      },
      "outputs": [],
      "source": [
        "# @title Evaluate a new metric\n",
        "\n",
        "# New metrics can be included in the comparison of existing metrics using the\n",
        "# 'extern_metrics' argument to GetCorrelations(). To demonstrate this, we'll\n",
        "# create and evaluate a new metric consisting of the average of the top 3\n",
        "# metrics in the system-level Pearson ranking.\n",
        "\n",
        "# The result is a slight, non-significant, improvement over C-SPECpn, the metric\n",
        "# with highest Pearson correlation. (The '*' before the new metric indicates\n",
        "# that it isn't recognized as a primary submission.)\n",
        "\n",
        "evs = all_evs['wmt21.news/en-de']\n",
        "\n",
        "# Create the new metric\n",
        "top3_metrics = ['C-SPECpn-refC', 'COMET-QE-MQM_2021-src', 'bleurt-20-refC']\n",
        "sys_scores = {}\n",
        "for sys_name in evs.sys_names:\n",
        "  if sys_name == 'refC': continue\n",
        "  scores = np.array([evs.Scores('sys', m)[sys_name] for m in top3_metrics])\n",
        "  sys_scores[sys_name] = scores.mean(axis=0)\n",
        "\n",
        "# Run the comparison with the new metric included via the 'extern_metrics'\n",
        "# argument.\n",
        "extras = {'top3_avg-refC': sys_scores}\n",
        "corrs = data.GetCorrelations(\n",
        "    evs, 'sys', {evs.std_ref}, {'refB'}, include_human, include_outliers,\n",
        "    gold_name, primary_metrics, domain, extern_metrics=extras)\n",
        "ranks, matrix = data.CompareMetrics(\n",
        "    corrs, scipy.stats.pearsonr, 'none', k, psd, pval)\n",
        "\n",
        "data.PrintMetricComparison(ranks, matrix, pval, evs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti_N-stPqmN4"
      },
      "outputs": [],
      "source": [
        "# @title Evaluate a new metric using global accuracy\n",
        "\n",
        "# This requires a bit more work, since we have to produce results for multiple\n",
        "# evalsets. As before, the result is a slight gain over the best single metric\n",
        "# (note that the averaged metrics aren't quite the top 3 for the global accuracy\n",
        "# task).\n",
        "\n",
        "# Create the new metric, one instance per input evalset\n",
        "top3_metrics = ['C-SPECpn-\u003cREF\u003e', 'COMET-QE-MQM_2021-src', 'bleurt-20-\u003cREF\u003e']\n",
        "extras_list = []\n",
        "for evs in evs_list:\n",
        "  top3 = [m.replace('\u003cREF\u003e', evs.std_ref) for m in top3_metrics]\n",
        "  sys_scores = {}\n",
        "  for sys_name in evs.sys_names:\n",
        "    if sys_name == evs.std_ref: continue\n",
        "    scores = np.array([evs.Scores('sys', m)[sys_name] for m in top3])\n",
        "    sys_scores[sys_name] = scores.mean(axis=0)\n",
        "  extras_list.append({f'top3_avg-{evs.std_ref}': sys_scores})\n",
        "\n",
        "# Run the comparison with the new metric included via the 'extern_metrics_list'\n",
        "# argument.\n",
        "ranks, matrix = data.CompareMetricsWithGlobalAccuracy(\n",
        "    evs_list, main_refs, close_refs, include_human, include_outliers,\n",
        "    gold_name, primary_metrics, domain, k, psd, pval,\n",
        "    extern_metrics_list=extras_list)\n",
        "\n",
        "data.PrintMetricComparison(ranks, matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isPvYjjHU7SA"
      },
      "source": [
        "# Ranking metrics using the task interface\n",
        "\n",
        "This is a higher-level interface designed to make it more convenient to compare\n",
        "a set of metrics using various different criteria called 'tasks'. The following\n",
        "code uses this interface to roughly duplicate the comparisons in the previous\n",
        "section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJAgSNrEz03r"
      },
      "outputs": [],
      "source": [
        "# @title Define a set of tasks\n",
        "\n",
        "# Create TaskSets from dicts that specify attribute/value-list combinations,\n",
        "# along with fixed assignments to other attributes. Concatenate these into a\n",
        "# single TaskSet.\n",
        "\n",
        "k = 1  # Use only a single random draw for demo.\n",
        "lang0 = {'test_set': ['wmt21.news'], 'lang': ['en-de,en-ru,zh-en']}\n",
        "langs = {'test_set': ['wmt21.news'], 'lang': ['en-de', 'en-ru', 'zh-en']}\n",
        "\n",
        "taskset = tasks.TaskSet(\n",
        "    lang0, corr_fcn='accuracy', close_refs=[{'refB'}, set(), set()], k=k)\n",
        "taskset += tasks.TaskSet(langs, level='sys', corr_fcn='pearson', k=k)\n",
        "taskset += tasks.TaskSet(langs, level='seg', corr_fcn='pearson', k=k)\n",
        "taskset += tasks.TaskSet(\n",
        "    langs, level='seg', avg_by='item', corr_fcn='KendallWithTiesOpt',\n",
        "    perm_test='pairs', corr_fcn_args={'sample_rate':0.01}, k=k)\n",
        "\n",
        "# A TaskSet is just a list of Tasks, so we can make arbitrary changes to\n",
        "# attribute values. In this case, set the correct close_refs for en-de tasks.\n",
        "\n",
        "for task in taskset:\n",
        "  if task.lang == 'en-de': task.close_refs = {'refB'}\n",
        "\n",
        "# Print task 'names' (attribute/value strings in canonical order).\n",
        "\n",
        "for t in taskset:\n",
        "  print(t.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA_oY6dq0D9m"
      },
      "outputs": [],
      "source": [
        "# @title Run the tasks\n",
        "\n",
        "# This first loads the necessary data, then runs each task in sequence to\n",
        "# produce a TaskSetResults object. Subsequent runs re-use the loaded data.\n",
        "\n",
        "results = taskset.Run()  # Takes about 5 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ouFanD20HzV"
      },
      "outputs": [],
      "source": [
        "# @title Print raw task results\n",
        "\n",
        "for result in results:\n",
        "  print(result.name)\n",
        "  print(result.Str())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC20eqtG0Mlh"
      },
      "outputs": [],
      "source": [
        "# @title Average ranks for metrics\n",
        "\n",
        "# To combine the performance of metrics across tasks, we average their task\n",
        "# ranks. The tasks are weighted to ensure that the total mass for important\n",
        "# attributes is evenly distributed among the different values those attributes\n",
        "# take on.\n",
        "weights = results.AssignWeights(tasks.Attributes())\n",
        "global_ranks = results.AverageRanks(weights)\n",
        "\n",
        "# It is also interesting to compare the metric performance on different subsets\n",
        "# of tasks, for instance split by language.\n",
        "ranks_by_lp = {}\n",
        "for val, subset in results.SplitByAttr('lang').items():\n",
        "  weights = subset.AssignWeights(tasks.Attributes())\n",
        "  ranks_by_lp[val] = subset.AverageRanks(weights)\n",
        "\n",
        "# Print out the comparison, with global ranks first, followed by a breakdown\n",
        "# by language pair. We only show metrics that are in the intersection of all\n",
        "# tasks.\n",
        "langs = [' all ' if lp == 'en-de,en-ru,zh-en' else lp for lp in ranks_by_lp]\n",
        "print(''.rjust(24), 'global', ' '.join(langs))\n",
        "for metric, rank in global_ranks.items():\n",
        "  ranks_for_metric = [rank] + [d[metric] for d in ranks_by_lp.values()]\n",
        "  print(f'{metric:\u003c25}', ' '.join(f'{r:5.2f}' for r in ranks_for_metric))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "mt_metrics_eval.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1gXA-HQKMF6G4IdrUob8hPnbVm6_rsfeX",
          "timestamp": 1656987947120
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
